"""
Script de scraping initial pour Avito Maroc (section voitures d'occasion)
--------------------------------------------------------
Ce script permet d'extraire les annonces de voitures d'occasion sur le site Avito.ma.
Il collecte les informations de base pour chaque annonce sur plusieurs pages:
- Titre de l'annonce
- Prix
- Date de publication
- Ann√©e du v√©hicule
- Type de carburant
- Type de transmission
- Type de vendeur (particulier ou professionnel)
- URL de l'annonce

Fonctionnalit√©s:
- Navigation automatique entre les pages sp√©cifi√©es par l'utilisateur
- Conversion des dates relatives en dates absolues
- Cr√©ation d'identifiants uniques pour chaque annonce
- Sauvegarde des donn√©es collect√©es dans un fichier CSV

Utilisation:
1. Ex√©cutez le script
2. Entrez la plage de pages √† scraper lorsque demand√©
3. Les r√©sultats seront sauvegard√©s dans ../data/avito/

D√©pendances: selenium, webdriver_manager, datetime, re, csv, os, time
"""


# initial_scraper.py
import time
import csv
import os
import re
from datetime import datetime, timedelta
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from webdriver_manager.chrome import ChromeDriverManager


def setup_driver():
    """Configure et initialise le driver Selenium."""
    options = Options()
    options.add_argument("--headless")  # Ex√©cuter sans interface graphique
    options.add_argument("--disable-gpu")
    options.add_argument("--window-size=1920,1080")
    options.add_argument("--log-level=3")  # R√©duire les logs inutiles

    service = Service(ChromeDriverManager().install())
    driver = webdriver.Chrome(service=service, options=options)
    return driver


def convert_relative_date(relative_date):
    """Convertit une date relative en date exacte."""
    now = datetime.now()

    # Cas "il y a quelques instants" ‚Üí prendre l'heure actuelle
    if "quelques instants" in relative_date.lower():
        return now.strftime("%Y-%m-%d %H:%M:%S")  # Date et heure actuelles

    # Extraction du nombre (ex: "5" dans "il y a 5 minutes")
    match = re.search(r'(\d+)', relative_date)
    if match:
        num = int(match.group(1))  # Convertir en entier
    else:
        return "Date inconnue"  # Aucun nombre trouv√©

    # Gestion des cas sp√©cifiques
    if "minute" in relative_date:
        exact_date = now - timedelta(minutes=num)
        return exact_date.strftime("%Y-%m-%d %H:%M:%S")  # Garde l'heure

    elif "heure" in relative_date:
        exact_date = now - timedelta(hours=num)
        return exact_date.strftime("%Y-%m-%d %H:%M:%S")  # Garde l'heure

    elif "jour" in relative_date:
        exact_date = now - timedelta(days=num)
        return exact_date.strftime("%Y-%m-%d")  # Supprime l'heure

    elif "mois" in relative_date:
        exact_date = now - timedelta(days=30 * num)  # Approximation
        return exact_date.strftime("%Y-%m-%d")  # Supprime l'heure

    elif "an" in relative_date:
        exact_date = now - timedelta(days=365 * num)  # Approximation
        return exact_date.strftime("%Y-%m-%d")  # Supprime l'heure

    else:
        return "Date inconnue"  # Cas non pr√©vu


def create_folder_name(title, idx):
    """Cr√©e un nom de dossier valide pour stocker les images d'une annonce."""
    # Nettoyer le titre pour obtenir un nom de dossier valide
    folder_name = re.sub(r'[^\w\s-]', '', title)  # Supprimer les caract√®res non alphanum√©riques
    folder_name = re.sub(r'\s+', '_', folder_name)  # Remplacer les espaces par des underscores
    folder_name = folder_name[:50]  # Limiter la longueur
    
    # Ajouter l'ID pour garantir l'unicit√©
    folder_name = f"{idx}_{folder_name}"
    
    return folder_name


def scrape_avito(start_page, end_page):
    """Scrape the car listings on Avito for pages within the specified range."""
    base_url = "https://www.avito.ma/fr/maroc/voitures_d_occasion-%C3%A0_vendre"
    driver = setup_driver()
    
    data = []
    listing_id_counter = 1  # Initialize the global ID counter

    # Loop through pages in the specified range
    for page in range(start_page, end_page + 1):
        url = f"{base_url}?o={page}"
        print(f"üîé Scraping page {page} of {end_page}: {url}")
        
        driver.get(url)
        driver.set_page_load_timeout(180)  # Increase timeout duration

        # Wait for the page to load correctly
        try:
            WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.CLASS_NAME, "sc-1nre5ec-1")))
        except Exception as e:
            print(f"‚ùå Timeout: Impossible de charger la page {page} ({e})")
            continue  # Try next page instead of breaking entirely

        try:
            # Ensure all content is loaded by scrolling to the bottom
            driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
            time.sleep(3)  # Wait for additional content to load

            # Find the main container
            main_container = driver.find_element(By.CLASS_NAME, "sc-1nre5ec-1")

            # Get all listings on the page
            listings = main_container.find_elements(By.CSS_SELECTOR, "a.sc-1jge648-0.jZXrfL")

            if not listings:
                print(f"‚ùå Aucune annonce trouv√©e sur la page {page} ! Passage √† la page suivante.")
                continue  # Continue to next page

            print(f"‚úÖ {len(listings)} annonces trouv√©es sur la page {page} !")

            # Iterate through the listings
            for listing in listings:
                try:
                    # Title
                    title = listing.find_element(By.CSS_SELECTOR, "p.sc-1x0vz2r-0.iHApav").text.strip() if listing.find_elements(By.CSS_SELECTOR, "p.sc-1x0vz2r-0.iHApav") else "N/A"

                    # Price
                    price = listing.find_element(By.CSS_SELECTOR, "p.sc-1x0vz2r-0.dJAfqm").text.strip() if listing.find_elements(By.CSS_SELECTOR, "p.sc-1x0vz2r-0.dJAfqm") else "Prix non sp√©cifi√©"

                    # Publication date
                    pub_date_raw = listing.find_element(By.CSS_SELECTOR, "p.sc-1x0vz2r-0.layWaX").text.strip() if listing.find_elements(By.CSS_SELECTOR, "p.sc-1x0vz2r-0.layWaX") else "N/A"
                    pub_date = convert_relative_date(pub_date_raw)  # Convert to exact date

                    # Year
                    year = listing.find_element(By.XPATH, ".//span[contains(text(),'20')]").text.strip() if listing.find_elements(By.XPATH, ".//span[contains(text(),'20')]") else "N/A"

                    # Fuel type
                    fuel_type = listing.find_element(By.XPATH, ".//span[contains(text(),'Essence') or contains(text(),'Diesel') or contains(text(),'Hybride') or contains(text(),'√âlectrique')]").text.strip() if listing.find_elements(By.XPATH, ".//span[contains(text(),'Essence') or contains(text(),'Diesel') or contains(text(),'Hybride') or contains(text(),'√âlectrique')]") else "N/A"

                    # Transmission
                    transmission = listing.find_element(By.XPATH, ".//span[contains(text(),'Automatique') or contains(text(),'Manuelle')]").text.strip() if listing.find_elements(By.XPATH, ".//span[contains(text(),'Automatique') or contains(text(),'Manuelle')]") else "N/A"

                    # Listing link
                    link = listing.get_attribute("href") if listing.get_attribute("href") else "N/A"

                    # Creator
                    creator = "Particulier"
                    try:
                        creator_element = listing.find_element(By.CSS_SELECTOR, "p.sc-1x0vz2r-0.hNCqYw.sc-1wnmz4-5.dXzQnB")
                        creator = creator_element.text.strip() if creator_element else "Particulier"
                    except:
                        pass  # If no name found, set to "Particulier" by default

                    # Create folder name for this listing
                    folder_name = create_folder_name(title, listing_id_counter)

                    # Save data
                    data.append([listing_id_counter, title, price, pub_date, year, fuel_type, transmission, creator, link, folder_name])

                    listing_id_counter += 1  # Increment the global counter after each listing

                except Exception as e:
                    print(f"‚ö†Ô∏è Erreur avec l'annonce sur la page {page}: {e}")

        except Exception as e:
            print(f"‚ùå Erreur lors de l'extraction de la page {page}: {e}")

    driver.quit()

    return data


def save_to_csv(data, filename):
    """Sauvegarde les donn√©es dans un fichier CSV dans ../data/avito/."""
    output_folder = os.path.join("..", "data", "avito")  # New directory: ../data/avito/
    os.makedirs(output_folder, exist_ok=True)  # Create if not exists
    output_file = os.path.join(output_folder, filename)

    with open(output_file, "w", newline="", encoding="utf-8") as file:
        writer = csv.writer(file)
        writer.writerow(["ID", "Titre", "Prix", "Date de publication", "Ann√©e", "Type de carburant", "Transmission", "Cr√©ateur", "URL de l'annonce", "Dossier d'images"])
        writer.writerows(data)

    print(f"‚úÖ Donn√©es sauvegard√©es dans {output_file}")
    return output_file


def get_page_range():
    """Demander √† l'utilisateur de sp√©cifier une plage de pages √† scraper."""
    while True:
        try:
            print("\nVeuillez sp√©cifier la plage de pages √† scraper:")
            start_page = int(input("Page de d√©but: "))
            end_page = int(input("Page de fin: "))
            
            if start_page <= 0 or end_page <= 0:
                print("‚ö†Ô∏è Les num√©ros de page doivent √™tre positifs.")
                continue
                
            if start_page > end_page:
                print("‚ö†Ô∏è La page de d√©but doit √™tre inf√©rieure ou √©gale √† la page de fin.")
                continue
                
            return start_page, end_page
        except ValueError:
            print("‚ö†Ô∏è Veuillez entrer des nombres valides.")


def main():
    """Main function to run the initial scraper."""
    print("üöó Starting Avito car listings scraper - Initial Phase...")
    
    # Get page range from user
    start_page, end_page = get_page_range()
    print(f"\nüìã Scraping car listings from page {start_page} to page {end_page}...")
    
    # Scrape specified pages
    basic_data = scrape_avito(start_page, end_page)
    
    if basic_data is None or len(basic_data) == 0:
        print("‚ùå No data found. Exiting program.")
        return
    
    # Save basic listings to CSV
    file_suffix = f"p{start_page}-p{end_page}"
    basic_csv = save_to_csv(basic_data, f"avito_listings_{file_suffix}.csv")
    
    print("\n‚úÖ INITIAL SCRAPING COMPLETED SUCCESSFULLY!")
    print(f"Found {len(basic_data)} listings across {end_page - start_page + 1} pages.")
    print(f"Basic listings saved to: {basic_csv}")
    print("Run the detail_scraper.py script next to gather detailed information.")


if __name__ == "__main__":
    main()