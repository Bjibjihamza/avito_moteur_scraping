"""
Moteur.ma Car Listings Scraper

This script is used to scrape car listings from the `Moteur.ma` website (a Moroccan marketplace for used cars).
The scraper collects basic information about car listings such as:
- Title
- Price
- Year of manufacture
- Fuel type
- Transmission type
- City of the seller
- URL of the listing

In addition, detailed information is scraped from the individual listing pages, including:
- Mileage
- Brand and model
- Equipment and features
- Seller's location
- Images associated with the listing

The collected data is saved into a CSV file (`moteur_complete.csv`) and the images are saved in a dedicated folder structure.

Main Features:
- Scrapes car listings from the homepage
- Visits detailed pages for each listing
- Downloads images and organizes them into specific folders
- Collects structured data and saves it into a CSV file
- Saves additional information such as equipment and seller's location

Dependencies:
- Selenium (for web scraping)
- Requests (for downloading images)
- WebDriver Manager (for handling browser drivers)
- Pandas (for saving data into CSV files)

Usage:
1. Run the script to scrape car listings from Moteur.ma.
2. The data will be saved in the `../data/moteur/` directory as a CSV file and images will be stored in the corresponding subfolder.
"""
 

import os
import re
import time
import json
import requests
import unicodedata
import pandas as pd
from datetime import datetime
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from webdriver_manager.chrome import ChromeDriverManager

# Configuration des r√©pertoires
DATA_DIR = "../data/moteur"
IMAGES_DIR = os.path.join(DATA_DIR, "images")
os.makedirs(DATA_DIR, exist_ok=True)
os.makedirs(IMAGES_DIR, exist_ok=True)



def setup_driver():
    """Configure et initialise le driver Selenium."""
    options = Options()
    options.add_argument("--headless")
    options.add_argument("--no-sandbox")
    options.add_argument("--disable-dev-shm-usage")
    options.add_argument("--disable-blink-features=AutomationControlled")
    options.add_argument("--window-size=1920,1080")
    options.add_argument("--log-level=3")
    options.add_argument("user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36")

    service = Service(ChromeDriverManager().install())
    driver = webdriver.Chrome(service=service, options=options)
    return driver

def sanitize_filename(filename):
    """Nettoie un nom de fichier pour qu'il soit valide sur le syst√®me d'exploitation."""
    # Remplacer les caract√®res non-alphanum√©riques par des underscores
    filename = re.sub(r'[^\w\s-]', '_', filename)
    # Normaliser les caract√®res accentu√©s
    filename = unicodedata.normalize('NFKD', filename).encode('ASCII', 'ignore').decode('ASCII')
    # Remplacer les espaces par des underscores
    filename = re.sub(r'\s+', '_', filename)
    return filename

def download_image(url, folder_path, index):
    """T√©l√©charge une image √† partir d'une URL avec des en-t√™tes am√©lior√©s."""
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
    }
    try:
        print(f"T√©l√©chargement de {url} vers {folder_path}")
        response = requests.get(url, headers=headers, timeout=15)
        if response.status_code == 200:
            file_extension = url.split('.')[-1]
            if '?' in file_extension:
                file_extension = file_extension.split('?')[0]
            if not file_extension or len(file_extension) > 5:
                file_extension = "jpg"  # Extension par d√©faut si probl√®me
            image_path = os.path.join(folder_path, f"image_{index}.{file_extension}")
            with open(image_path, 'wb') as f:
                f.write(response.content)
            print(f"‚úÖ Image enregistr√©e : {image_path}")
            return True
        else:
            print(f"‚ùå Erreur HTTP {response.status_code} pour {url}")
        return False
    except Exception as e:
        print(f"‚ö†Ô∏è Erreur lors du t√©l√©chargement de l'image {url}: {e}")
        return False

def extract_id_from_url(url):
    """Extrait l'ID de l'annonce √† partir de l'URL."""
    match = re.search(r"/detail-annonce/(\d+)/", url)
    return match.group(1) if match else "N/A"

def scrape_listings_page(driver, page_number):
    """Scrape la page des annonces."""
    BASE_URL = "https://www.moteur.ma/fr/voiture/achat-voiture-occasion/"
    offset = (page_number - 1) * 30
    page_url = f"{BASE_URL}{offset}" if offset > 0 else BASE_URL

    print(f"üîé Scraping page {page_number}: {page_url}")
    driver.get(page_url)

    try:
        WebDriverWait(driver, 20).until(
            EC.presence_of_element_located((By.CLASS_NAME, "row-item"))
        )
    except:
        print(f"‚ùå Aucune annonce trouv√©e sur la page {page_number} !")
        return []

    car_elements = driver.find_elements(By.CLASS_NAME, "row-item")
    print(f"‚úÖ {len(car_elements)} annonces trouv√©es sur la page {page_number} !")

    data = []

    for car in car_elements:
        try:
            title_element = car.find_element(By.CLASS_NAME, "title_mark_model")
            title = title_element.text.strip() if title_element else "N/A"

            try:
                link_element = car.find_element(By.XPATH, ".//h3[@class='title_mark_model']/a")
                link = link_element.get_attribute("href") if link_element else "N/A"
                ad_id = extract_id_from_url(link)
            except:
                link, ad_id = "N/A", "N/A"

            try:
                price_element = car.find_element(By.CLASS_NAME, "PriceListing")
                price = price_element.text.strip()
            except:
                price = "N/A"

            meta_elements = car.find_elements(By.TAG_NAME, "li")
            year = "N/A"
            city = "N/A"
            fuel = "N/A"

            # Liste des valeurs √† ignorer comme fausses villes
            forbidden_values = ["Appeler pour le prix", "Se faire rappeler", "Booster l'annonce", ""]

            for li in meta_elements:
                text = li.text.strip()

                if re.match(r"^(19|20)\d{2}$", text):
                    year = text
                elif text.lower() in ["essence", "diesel", "hybride", "√©lectrique"]:
                    fuel = text.capitalize()
                elif city == "N/A" and text not in forbidden_values:
                    city = text

            if city in forbidden_values or city == "N/A":
                print(f"‚ö†Ô∏è Ville douteuse pour l'annonce ID {ad_id} : '{city}' - {link}")

            data.append({
                "ID": ad_id,
                "Titre": title,
                "Prix": price,
                "Ann√©e": year,
                "Type de carburant": fuel,
                "Ville": city,
                "URL de l'annonce": link
            })

        except Exception as e:
            print(f"‚ö†Ô∏è Erreur avec une annonce: {e}")

    time.sleep(3)
    return data

def scrape_detail_page(driver, url, ad_id, title, price):
    """Scrape les d√©tails d'une annonce sp√©cifique."""
    try:
        # Acc√©der √† la page de d√©tail
        driver.get(url)
        time.sleep(3)  # Attendre le chargement de la page
        
        # Cr√©er un nom de dossier unique pour les images
        folder_name = f"{ad_id}_{sanitize_filename(title)}"
        
        # Cr√©er un dossier pour les images de cette annonce
        folder_path = os.path.join(IMAGES_DIR, folder_name)
        if not os.path.exists(folder_path):
            os.makedirs(folder_path)
            print(f"üìÇ Dossier cr√©√© : {folder_path}")
        
        # Initialiser avec des valeurs par d√©faut
        location = "N/A"
        mileage = "N/A"
        brand = "N/A"
        model = "N/A"
        doors = "N/A"
        first_hand = "N/A"
        fiscal_power = "N/A"
        equipment_text = "N/A"
        seller_city = "N/A"
        dedouane = "N/A"
        transmission = "N/A"
        fuel_type = "N/A"
        creator = "N/A"
        
        # Extraction du cr√©ateur
        try:
            # Rechercher le lien avec l'ic√¥ne "megaphone"
            creator_element = driver.find_element(By.XPATH, "//a[contains(., 'icon-normal-megaphone')]")
            if creator_element:
                creator = creator_element.text.strip()
            
            # Si la m√©thode ci-dessus √©choue, essayer une autre approche
            if creator == "N/A":
                creator_element = driver.find_element(By.XPATH, "//div[@class='actions block_tele']//li/a[i[contains(@class, 'icon-normal-megaphone')]]")
                creator = creator_element.text.strip()
        except Exception as e:
            print(f"Erreur extraction du cr√©ateur: {e}")
            # Troisi√®me tentative avec un XPath plus pr√©cis
            try:
                creator_element = driver.find_element(By.XPATH, "//div[@class='block-inner block-detail-ad']//div[@class='actions block_tele']//a[contains(@href, 'stock-professionnel')]")
                creator = creator_element.text.strip()
            except Exception as e2:
                print(f"Erreur extraction du cr√©ateur (m√©thode alternative): {e2}")
        
        # Extraction de la transmission
        try:
            transmission_element = driver.find_element(By.XPATH, "//span[contains(text(), 'Boite de vitesses')]/following-sibling::span")
            transmission = transmission_element.text.strip()
        except Exception as e:
            print(f"Erreur extraction directe de transmission: {e}")
        
        # R√©cup√©rer les d√©tails du v√©hicule
        detail_lines = driver.find_elements(By.CLASS_NAME, "detail_line")
        
        for line in detail_lines:
            try:
                spans = line.find_elements(By.TAG_NAME, "span")
                if len(spans) >= 2:
                    key = spans[0].text.strip()
                    value = spans[1].text.strip()
                    
                    if "Kilom√©trage" in key:
                        mileage = value
                    elif "Boite de vitesses" in key:
                        transmission = value
                    elif "Carburant" in key:
                        fuel_type = value
                    elif "Puissance fiscale" in key:
                        fiscal_power = value
                    elif "Nombre de portes" in key:
                        doors = value
                    elif "Premi√®re main" in key:
                        first_hand = value
                    elif "V√©hicule d√©douan√©" in key:
                        dedouane = value
            except Exception as e:
                print(f"Erreur lors de l'extraction d'une ligne de d√©tail: {e}")
        
        # Extraction de la description (√©quipements)
        try:
            description_element = driver.find_element(By.CSS_SELECTOR, "div.options div.col-md-12")
            equipment_text = description_element.text.strip()
        except Exception as e:
            print(f"Erreur extraction description: {e}")
        
        # Extraction de la ville
        try:
            city_element = driver.find_element(By.XPATH, "//a[contains(@href, 'ville')]")
            seller_city = city_element.text.strip()
            location = seller_city
        except Exception as e:
            print(f"Erreur extraction ville: {e}")
        
        # Extraction des images
        image_count = 0
        try:
            image_elements = driver.find_elements(By.CSS_SELECTOR, "img[data-u='image']")
            for index, img in enumerate(image_elements):
                img_url = img.get_attribute("src")
                if img_url and "http" in img_url:
                    success = download_image(img_url, folder_path, index + 1)
                    if success:
                        image_count += 1
        except Exception as e:
            print(f"Erreur lors de l'extraction des images: {e}")
        
        # Extraction de la marque et du mod√®le depuis le titre
        try:
            title_parts = title.split()
            if len(title_parts) >= 2:
                brand = title_parts[0].upper()
                model = title_parts[1].capitalize()
        except Exception as e:
            print(f"Erreur lors de l'extraction de la marque et du mod√®le depuis le titre: {e}")
        
        detail_data = {
            "ID": ad_id,
            "Titre": title,
            "Prix": price,
            "Date de publication": datetime.now().strftime("%Y-%m-%d"),
            "Type de carburant": fuel_type,
            "Transmission": transmission,
            "Cr√©ateur": creator,
            "Secteur": location,
            "Kilom√©trage": mileage,
            "Marque": brand,
            "Mod√®le": model,
            "Nombre de portes": doors,
            "Premi√®re main": first_hand,
            "Puissance fiscale": fiscal_power,
            "√âquipements": equipment_text,
            "Ville du vendeur": seller_city,
            "Dossier d'images": folder_name,
            "D√©douan√©": dedouane
        }
        
        return detail_data
        
    except Exception as e:
        print(f"‚ùå Erreur lors du scraping de la page {url}: {e}")
        return {
            "ID": ad_id,
            "Titre": title,
            "Prix": price,
            "Date de publication": datetime.now().strftime("%Y-%m-%d"),
            "Type de carburant": "N/A",
            "Transmission": "N/A",
            "Cr√©ateur": "N/A",
            "Secteur": "N/A",
            "Kilom√©trage": "N/A",
            "Marque": "N/A",
            "Mod√®le": "N/A",
            "Nombre de portes": "N/A",
            "Premi√®re main": "N/A",
            "Puissance fiscale": "N/A",
            "√âquipements": "N/A",
            "Ville du vendeur": "N/A",
            "Dossier d'images": "N/A",
            "D√©douan√©": "N/A"
        }

def main():
    """Fonction principale qui ex√©cute le scraper complet."""
    print("üöó D√©marrage du scraper complet Moteur.ma...")
    
    driver = setup_driver()
    
    
    try:
        # √âtape 1: Scraper la liste des annonces
        print("\nüìã R√©cup√©ration des annonces de la page 1...")
        listings_data = scrape_listings_page(driver, 1)
        
        if not listings_data:
            print("‚ùå Aucune annonce trouv√©e. Arr√™t du programme.")
            driver.quit()
            return
            
        print(f"‚úÖ {len(listings_data)} annonces r√©cup√©r√©es.")
        
        # √âtape 2: R√©cup√©rer les d√©tails pour chaque annonce
        print("\nüîç R√©cup√©ration des d√©tails pour chaque annonce...")
        detailed_data = []
        
        for idx, listing in enumerate(listings_data, start=1):
            print(f"\n‚û°Ô∏è Traitement de l'annonce {idx}/{len(listings_data)}: {listing['Titre']}")
            
            # R√©cup√©rer les d√©tails de l'annonce
            detail = scrape_detail_page(
                driver, 
                listing["URL de l'annonce"], 
                listing['ID'], 
                listing['Titre'], 
                listing['Prix']
            )
            
            
            detailed_data.append(detail)
            time.sleep(2)  # Pause entre chaque annonce
        
        # √âtape 3: Enregistrer les donn√©es dans un fichier CSV final
        output_file = os.path.join(DATA_DIR, f"moteur_complete.csv")
        
        # Cr√©er un DataFrame et l'enregistrer
        df = pd.DataFrame(detailed_data)
        df.to_csv(output_file, index=False, encoding="utf-8-sig")
        
        print("\n‚úÖ SCRAPING TERMIN√â AVEC SUCC√àS!")
        print(f"üìä {len(detailed_data)} annonces compl√®tes r√©cup√©r√©es.")
        print(f"üìÑ Donn√©es enregistr√©es dans: {output_file}")
        print(f"üñºÔ∏è Images t√©l√©charg√©es dans: {IMAGES_DIR}")
        
    except Exception as e:
        print(f"‚ùå Erreur globale: {e}")
    finally:

        
        driver.quit()
        print("üèÅ Programme termin√©.")

if __name__ == "__main__":
    main()