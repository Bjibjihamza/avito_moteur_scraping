"""
Moteur.ma Car Listings Scraper

This script is used to scrape car listings from the `Moteur.ma` website (a Moroccan marketplace for used cars).
The scraper collects basic information about car listings such as:
- Title
- Price
- Year of manufacture
- Fuel type
- Transmission type
- City of the seller
- URL of the listing

In addition, detailed information is scraped from the individual listing pages, including:
- Mileage
- Brand and model
- Equipment and features
- Seller's location
- Images associated with the listing

The collected data is saved into a CSV file (`moteur_complete.csv`) and the images are saved in a dedicated folder structure.

Main Features:
- Scrapes car listings from the homepage
- Visits detailed pages for each listing
- Downloads images and organizes them into specific folders
- Collects structured data and saves it into a CSV file
- Saves additional information such as equipment and seller's location

Dependencies:
- Selenium (for web scraping)
- Requests (for downloading images)
- WebDriver Manager (for handling browser drivers)
- Pandas (for saving data into CSV files)

Usage:
1. Run the script to scrape car listings from Moteur.ma.
2. The data will be saved in the `../data/moteur/` directory as a CSV file and images will be stored in the corresponding subfolder.
"""
 

import os
import re
import time
import json
import requests
import unicodedata
import pandas as pd
from datetime import datetime
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from webdriver_manager.chrome import ChromeDriverManager

# Configuration des rÃ©pertoires
DATA_DIR = "../data/moteur"
IMAGES_DIR = os.path.join(DATA_DIR, "images")
os.makedirs(DATA_DIR, exist_ok=True)
os.makedirs(IMAGES_DIR, exist_ok=True)



def setup_driver():
    """Configure et initialise le driver Selenium."""
    options = Options()
    options.add_argument("--headless")
    options.add_argument("--no-sandbox")
    options.add_argument("--disable-dev-shm-usage")
    options.add_argument("--disable-blink-features=AutomationControlled")
    options.add_argument("--window-size=1920,1080")
    options.add_argument("--log-level=3")
    options.add_argument("user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36")

    service = Service(ChromeDriverManager().install())
    driver = webdriver.Chrome(service=service, options=options)
    return driver

def sanitize_filename(filename):
    """Nettoie un nom de fichier pour qu'il soit valide sur le systÃ¨me d'exploitation."""
    # Remplacer les caractÃ¨res non-alphanumÃ©riques par des underscores
    filename = re.sub(r'[^\w\s-]', '_', filename)
    # Normaliser les caractÃ¨res accentuÃ©s
    filename = unicodedata.normalize('NFKD', filename).encode('ASCII', 'ignore').decode('ASCII')
    # Remplacer les espaces par des underscores
    filename = re.sub(r'\s+', '_', filename)
    return filename

def download_image(url, folder_path, index):
    """TÃ©lÃ©charge une image Ã  partir d'une URL avec des en-tÃªtes amÃ©liorÃ©s."""
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
    }
    try:
        print(f"TÃ©lÃ©chargement de {url} vers {folder_path}")
        response = requests.get(url, headers=headers, timeout=15)
        if response.status_code == 200:
            file_extension = url.split('.')[-1]
            if '?' in file_extension:
                file_extension = file_extension.split('?')[0]
            if not file_extension or len(file_extension) > 5:
                file_extension = "jpg"  # Extension par dÃ©faut si problÃ¨me
            image_path = os.path.join(folder_path, f"image_{index}.{file_extension}")
            with open(image_path, 'wb') as f:
                f.write(response.content)
            print(f"âœ… Image enregistrÃ©e : {image_path}")
            return True
        else:
            print(f"âŒ Erreur HTTP {response.status_code} pour {url}")
        return False
    except Exception as e:
        print(f"âš ï¸ Erreur lors du tÃ©lÃ©chargement de l'image {url}: {e}")
        return False

def extract_id_from_url(url):
    """Extrait l'ID de l'annonce Ã  partir de l'URL."""
    match = re.search(r"/detail-annonce/(\d+)/", url)
    return match.group(1) if match else "N/A"

def scrape_listings_page(driver, page_number):
    """Scrape la page des annonces."""
    BASE_URL = "https://www.moteur.ma/fr/voiture/achat-voiture-occasion/"
    offset = (page_number - 1) * 30
    page_url = f"{BASE_URL}{offset}" if offset > 0 else BASE_URL

    print(f"ğŸ” Scraping page {page_number}: {page_url}")
    driver.get(page_url)

    try:
        WebDriverWait(driver, 20).until(
            EC.presence_of_element_located((By.CLASS_NAME, "row-item"))
        )
    except:
        print(f"âŒ Aucune annonce trouvÃ©e sur la page {page_number} !")
        return []

    car_elements = driver.find_elements(By.CLASS_NAME, "row-item")
    print(f"âœ… {len(car_elements)} annonces trouvÃ©es sur la page {page_number} !")

    data = []

    for car in car_elements:
        try:
            title_element = car.find_element(By.CLASS_NAME, "title_mark_model")
            title = title_element.text.strip() if title_element else "N/A"

            try:
                link_element = car.find_element(By.XPATH, ".//h3[@class='title_mark_model']/a")
                link = link_element.get_attribute("href") if link_element else "N/A"
                ad_id = extract_id_from_url(link)
            except:
                link, ad_id = "N/A", "N/A"

            try:
                price_element = car.find_element(By.CLASS_NAME, "PriceListing")
                price = price_element.text.strip()
            except:
                price = "N/A"

            meta_elements = car.find_elements(By.TAG_NAME, "li")
            year = "N/A"
            city = "N/A"
            fuel = "N/A"

            # Liste des valeurs Ã  ignorer comme fausses villes
            forbidden_values = ["Appeler pour le prix", "Se faire rappeler", "Booster l'annonce", ""]

            for li in meta_elements:
                text = li.text.strip()

                if re.match(r"^(19|20)\d{2}$", text):
                    year = text
                elif text.lower() in ["essence", "diesel", "hybride", "Ã©lectrique"]:
                    fuel = text.capitalize()
                elif city == "N/A" and text not in forbidden_values:
                    city = text

            if city in forbidden_values or city == "N/A":
                print(f"âš ï¸ Ville douteuse pour l'annonce ID {ad_id} : '{city}' - {link}")

            data.append({
                "ID": ad_id,
                "Titre": title,
                "Prix": price,
                "AnnÃ©e": year,
                "Type de carburant": fuel,
                "Ville": city,
                "URL de l'annonce": link
            })

        except Exception as e:
            print(f"âš ï¸ Erreur avec une annonce: {e}")

    time.sleep(3)
    return data

def scrape_detail_page(driver, url, ad_id, title, price):
    """Scrape les dÃ©tails d'une annonce spÃ©cifique."""
    try:
        # AccÃ©der Ã  la page de dÃ©tail
        driver.get(url)
        time.sleep(3)  # Attendre le chargement de la page
        
        # CrÃ©er un nom de dossier unique pour les images
        folder_name = f"{ad_id}_{sanitize_filename(title)}"
        
        # CrÃ©er un dossier pour les images de cette annonce
        folder_path = os.path.join(IMAGES_DIR, folder_name)
        if not os.path.exists(folder_path):
            os.makedirs(folder_path)
            print(f"ğŸ“‚ Dossier crÃ©Ã© : {folder_path}")
        
        # Initialiser avec des valeurs par dÃ©faut
        location = "N/A"
        mileage = "N/A"
        brand = "N/A"
        model = "N/A"
        doors = "N/A"
        first_hand = "N/A"
        fiscal_power = "N/A"
        equipment_text = "N/A"
        seller_city = "N/A"
        dedouane = "N/A"
        transmission = "N/A"
        fuel_type = "N/A"
        creator = "N/A"
        
        # Extraction du crÃ©ateur
        try:
            # Rechercher le lien avec l'icÃ´ne "megaphone"
            creator_element = driver.find_element(By.XPATH, "//a[contains(., 'icon-normal-megaphone')]")
            if creator_element:
                creator = creator_element.text.strip()
            
            # Si la mÃ©thode ci-dessus Ã©choue, essayer une autre approche
            if creator == "N/A":
                creator_element = driver.find_element(By.XPATH, "//div[@class='actions block_tele']//li/a[i[contains(@class, 'icon-normal-megaphone')]]")
                creator = creator_element.text.strip()
        except Exception as e:
            print(f"Erreur extraction du crÃ©ateur: {e}")
            # TroisiÃ¨me tentative avec un XPath plus prÃ©cis
            try:
                creator_element = driver.find_element(By.XPATH, "//div[@class='block-inner block-detail-ad']//div[@class='actions block_tele']//a[contains(@href, 'stock-professionnel')]")
                creator = creator_element.text.strip()
            except Exception as e2:
                print(f"Erreur extraction du crÃ©ateur (mÃ©thode alternative): {e2}")
        
        # Extraction de la transmission
        try:
            transmission_element = driver.find_element(By.XPATH, "//span[contains(text(), 'Boite de vitesses')]/following-sibling::span")
            transmission = transmission_element.text.strip()
        except Exception as e:
            print(f"Erreur extraction directe de transmission: {e}")
        
        # RÃ©cupÃ©rer les dÃ©tails du vÃ©hicule
        detail_lines = driver.find_elements(By.CLASS_NAME, "detail_line")
        
        for line in detail_lines:
            try:
                spans = line.find_elements(By.TAG_NAME, "span")
                if len(spans) >= 2:
                    key = spans[0].text.strip()
                    value = spans[1].text.strip()
                    
                    if "KilomÃ©trage" in key:
                        mileage = value
                    elif "Boite de vitesses" in key:
                        transmission = value
                    elif "Carburant" in key:
                        fuel_type = value
                    elif "Puissance fiscale" in key:
                        fiscal_power = value
                    elif "Nombre de portes" in key:
                        doors = value
                    elif "PremiÃ¨re main" in key:
                        first_hand = value
                    elif "VÃ©hicule dÃ©douanÃ©" in key:
                        dedouane = value
            except Exception as e:
                print(f"Erreur lors de l'extraction d'une ligne de dÃ©tail: {e}")
        
        # Extraction de la description (Ã©quipements)
        try:
            description_element = driver.find_element(By.CSS_SELECTOR, "div.options div.col-md-12")
            equipment_text = description_element.text.strip()
        except Exception as e:
            print(f"Erreur extraction description: {e}")
        
        # Extraction de la ville
        try:
            city_element = driver.find_element(By.XPATH, "//a[contains(@href, 'ville')]")
            seller_city = city_element.text.strip()
            location = seller_city
        except Exception as e:
            print(f"Erreur extraction ville: {e}")
        
        # Extraction des images
        image_count = 0
        try:
            image_elements = driver.find_elements(By.CSS_SELECTOR, "img[data-u='image']")
            for index, img in enumerate(image_elements):
                img_url = img.get_attribute("src")
                if img_url and "http" in img_url:
                    success = download_image(img_url, folder_path, index + 1)
                    if success:
                        image_count += 1
        except Exception as e:
            print(f"Erreur lors de l'extraction des images: {e}")
        
        # Extraction de la marque et du modÃ¨le depuis le titre
        try:
            title_parts = title.split()
            if len(title_parts) >= 2:
                brand = title_parts[0].upper()
                model = title_parts[1].capitalize()
        except Exception as e:
            print(f"Erreur lors de l'extraction de la marque et du modÃ¨le depuis le titre: {e}")
        
        detail_data = {
            "ID": ad_id,
            "Titre": title,
            "Prix": price,
            "Date de publication": datetime.now().strftime("%Y-%m-%d"),
            "Type de carburant": fuel_type,
            "Transmission": transmission,
            "CrÃ©ateur": creator,
            "Secteur": location,
            "KilomÃ©trage": mileage,
            "Marque": brand,
            "ModÃ¨le": model,
            "Nombre de portes": doors,
            "PremiÃ¨re main": first_hand,
            "Puissance fiscale": fiscal_power,
            "Ã‰quipements": equipment_text,
            "Ville du vendeur": seller_city,
            "Dossier d'images": folder_name,
            "DÃ©douanÃ©": dedouane
        }
        
        return detail_data
        
    except Exception as e:
        print(f"âŒ Erreur lors du scraping de la page {url}: {e}")
        return {
            "ID": ad_id,
            "Titre": title,
            "Prix": price,
            "Date de publication": datetime.now().strftime("%Y-%m-%d"),
            "Type de carburant": "N/A",
            "Transmission": "N/A",
            "CrÃ©ateur": "N/A",
            "Secteur": "N/A",
            "KilomÃ©trage": "N/A",
            "Marque": "N/A",
            "ModÃ¨le": "N/A",
            "Nombre de portes": "N/A",
            "PremiÃ¨re main": "N/A",
            "Puissance fiscale": "N/A",
            "Ã‰quipements": "N/A",
            "Ville du vendeur": "N/A",
            "Dossier d'images": "N/A",
            "DÃ©douanÃ©": "N/A"
        }

def main():
    """Fonction principale qui exÃ©cute le scraper complet."""
    print("ğŸš— DÃ©marrage du scraper complet Moteur.ma...")
    
    driver = setup_driver()
    
    
    try:
        # Ã‰tape 1: Scraper la liste des annonces
        print("\nğŸ“‹ RÃ©cupÃ©ration des annonces de la page 1...")
        listings_data = scrape_listings_page(driver, 1)
        
        if not listings_data:
            print("âŒ Aucune annonce trouvÃ©e. ArrÃªt du programme.")
            driver.quit()
            return
            
        print(f"âœ… {len(listings_data)} annonces rÃ©cupÃ©rÃ©es.")
        
        # Ã‰tape 2: RÃ©cupÃ©rer les dÃ©tails pour chaque annonce
        print("\nğŸ” RÃ©cupÃ©ration des dÃ©tails pour chaque annonce...")
        detailed_data = []
        
        for idx, listing in enumerate(listings_data, start=1):
            print(f"\nâ¡ï¸ Traitement de l'annonce {idx}/{len(listings_data)}: {listing['Titre']}")
            
            # RÃ©cupÃ©rer les dÃ©tails de l'annonce
            detail = scrape_detail_page(
                driver, 
                listing["URL de l'annonce"], 
                listing['ID'], 
                listing['Titre'], 
                listing['Prix']
            )
            
            
            detailed_data.append(detail)
            time.sleep(2)  # Pause entre chaque annonce
        
        # Ã‰tape 3: Enregistrer les donnÃ©es dans un fichier CSV final
        output_file = os.path.join(DATA_DIR, f"moteur_complete.csv")
        
        # CrÃ©er un DataFrame et l'enregistrer
        df = pd.DataFrame(detailed_data)
        df.to_csv(output_file, index=False, encoding="utf-8-sig")
        
        print("\nâœ… SCRAPING TERMINÃ‰ AVEC SUCCÃˆS!")
        print(f"ğŸ“Š {len(detailed_data)} annonces complÃ¨tes rÃ©cupÃ©rÃ©es.")
        print(f"ğŸ“„ DonnÃ©es enregistrÃ©es dans: {output_file}")
        print(f"ğŸ–¼ï¸ Images tÃ©lÃ©chargÃ©es dans: {IMAGES_DIR}")
        
    except Exception as e:
        print(f"âŒ Erreur globale: {e}")
    finally:

        
        driver.quit()
        print("ğŸ Programme terminÃ©.")

if __name__ == "__main__":
    main()